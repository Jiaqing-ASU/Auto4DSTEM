{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4e6a4c1",
   "metadata": {},
   "source": [
    "# Import Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d10982",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "932063ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"3\"\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "#import hyperspy.api as hs\n",
    "import matplotlib.pyplot as plt\n",
    "#from dask_ml.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "from torch.autograd import Function\n",
    "from collections import OrderedDict\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import pickle\n",
    "import torch.autograd as autograd\n",
    "import matplotlib.gridspec as gridspec\n",
    "import os\n",
    "import h5py\n",
    "# from torchsummary import summary\n",
    "# import pytorch_model_summary as pms\n",
    "from tqdm import tqdm\n",
    "#import torch.tensor as tensor\n",
    "from scipy import ndimage\n",
    "import matplotlib.pyplot as plt\n",
    "import skimage\n",
    "from skimage.feature import peak_local_max\n",
    "from skimage import data, img_as_float,feature\n",
    "from skimage import io\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5deb7560",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pylab as pylab\n",
    "\n",
    "params = {'axes.titlesize':20,\n",
    "          'xtick.direction': 'in' ,\n",
    "          'ytick.direction' : 'in',\n",
    "          'xtick.top' : True,\n",
    "          'ytick.right' : True,\n",
    "          'ytick.labelsize':16,\n",
    "          'xtick.labelsize':16\n",
    "         }\n",
    "\n",
    "pylab.rcParams.update(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35088600",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1cb9d2f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import atomai as aoi\n",
    "#import kornia as K\n",
    "import cv2\n",
    "import scipy\n",
    "import argparse\n",
    "import skimage\n",
    "from skimage.util import random_noise\n",
    "from skimage import feature\n",
    "import glob\n",
    "from scipy import ndimage\n",
    "import scipy as sp\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "50886064",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2380d956",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import random_split\n",
    "from ray import tune\n",
    "from ray.tune import CLIReporter\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "from ray.air import session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "439d6b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# !{sys.executable} -m pip install -U \"ray[tune]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef58765",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8110a56",
   "metadata": {},
   "source": [
    "# Build two type of mask for two training process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dfb304e",
   "metadata": {},
   "source": [
    "## Set the mask function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "130fd141",
   "metadata": {},
   "source": [
    "# Loading Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e087d1c",
   "metadata": {},
   "source": [
    "# get dataset from: https://zenodo.org/records/10836435"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4663d22d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set data direction\n",
    "data_dir = os.path.abspath(\"Extremely_Noisy_4DSTEM_Strain_Mapping_Using_CC_ST_AE_Simulated/polycrystal_output4D.mat\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de65317",
   "metadata": {},
   "source": [
    "## Load data function 1 for first training process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d8b5d4",
   "metadata": {},
   "source": [
    "## Load label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8cc1a275",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_xx = np.load('Label_strain_xx.npy').reshape(-1)\n",
    "label_yy = np.load('Label_strain_yy.npy').reshape(-1)\n",
    "label_xy = np.load('Label_shear_xy.npy').reshape(-1)\n",
    "label_rotation = np.load('Label_rotation.npy').reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "641d8436",
   "metadata": {},
   "outputs": [],
   "source": [
    "supervised_label = np.stack((label_xx,label_yy,label_xy,label_rotation),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130c8df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "supervised_label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69300c35",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1,4,figsize=(16,4))\n",
    "ax[0].hist(supervised_label[:,0],200,range=[-0.03,0.03]);\n",
    "ax[1].hist(supervised_label[:,1],200,range=[-0.03,0.03]);\n",
    "ax[2].hist(supervised_label[:,2],200,range=[-0.03,0.03]);\n",
    "ax[3].hist(supervised_label[:,3],200,range=[2,3.2]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ed8656e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_4_process2(data_dir, label,w_bg=0.60):\n",
    "    \n",
    "    '''\n",
    "    \n",
    "        data_dir: path of the dataset\n",
    "        label_index: path of the pretrained rotation \n",
    "    \n",
    "    '''\n",
    "    \n",
    "    f = h5py.File(data_dir,'r')\n",
    "    op4d = f['output4D']\n",
    "    op4d = op4d[:,:,28:228,28:228]\n",
    "    op4d = np.transpose(op4d, (1, 0, 3, 2))\n",
    "    op4d = op4d.reshape(-1,200,200)\n",
    "    f.close()\n",
    "    \n",
    "    if w_bg == 0:\n",
    "        \n",
    "        noisy_data = op4d*1e5/4\n",
    "    \n",
    "    else:\n",
    "    \n",
    "        noisy_data = np.zeros([65536,200,200])\n",
    "        im=np.zeros([200,200])\n",
    "        counts_per_probe = 1e5\n",
    "        for i in tqdm(range(65536),leave=True,total=65536):\n",
    "            test_img = np.copy(op4d[i])\n",
    "            qx = np.fft.fftfreq( im.shape[0], d = 1)\n",
    "            qy = np.fft.fftfreq( im.shape[1], d = 1)\n",
    "            qya, qxa = np.meshgrid(qy, qx)\n",
    "            qxa = np.fft.fftshift(qxa)\n",
    "            qya = np.fft.fftshift(qya) \n",
    "            qra2 = qxa**2 + qya**2\n",
    "            im_bg = 1./( 1 + qra2 / 1e-2**2 )\n",
    "            im_bg = im_bg / np.sum(im_bg) \n",
    "            int_comb = test_img * (1 - w_bg) + im_bg * w_bg \n",
    "            int_noisy = np.random.poisson(int_comb * counts_per_probe) / counts_per_probe\n",
    "            int_noisy = int_noisy*1e5/4\n",
    "            noisy_data[i] = int_noisy\n",
    "        \n",
    "    del op4d\n",
    "    \n",
    "    noisy_data = noisy_data.reshape(-1,1,200,200)\n",
    "    if len(noisy_data) != len(label):\n",
    "        return('data and label do not pair')\n",
    "#     data_ = Dataset(noisy_data, label)\n",
    "    data_with_label = []\n",
    "    print('add image-rotation pair to whole dataset')\n",
    "    for i in tqdm(range(noisy_data.shape[0]), leave=True, total=noisy_data.shape[0]):\n",
    "        data_with_label.append([noisy_data[i], label[i]])\n",
    "    del(noisy_data)\n",
    "    \n",
    "    return data_with_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "437d6892",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ = load_data_4_process2(data_dir, supervised_label,w_bg=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "487be412",
   "metadata": {},
   "source": [
    "# Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8ff4f29d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class conv_block(nn.Module):\n",
    "    def __init__(self,t_size,n_step):\n",
    "        super(conv_block,self).__init__()\n",
    "        self.cov1d_1 = nn.Conv2d(t_size,t_size,3,stride=1,padding=1,padding_mode = 'zeros')\n",
    "        self.cov1d_2 = nn.Conv2d(t_size,t_size,3,stride=1,padding=1,padding_mode = 'zeros')\n",
    "        self.cov1d_3 = nn.Conv2d(t_size,t_size,3,stride=1,padding=1,padding_mode = 'zeros')\n",
    "        self.norm_3 = nn.LayerNorm(n_step)\n",
    "        self.relu_1 = nn.ReLU()\n",
    "        self.relu_2 = nn.ReLU()\n",
    "        self.relu_3 = nn.ReLU()\n",
    "#         self.relu_1 = nn.Tanh()\n",
    "#         self.relu_2 = nn.Tanh()\n",
    "#         self.relu_3 = nn.Tanh()\n",
    "        #self.drop = nn.Dropout(p=0.2)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x_input = x\n",
    "        out = self.cov1d_1(x)\n",
    "        out = self.relu_1(out)\n",
    "        out = self.cov1d_2(out)\n",
    "        out = self.relu_2(out)\n",
    "        out = self.cov1d_3(out)\n",
    "        out = self.norm_3(out)\n",
    "        out = self.relu_3(out)\n",
    "        #out = self.drop(out)\n",
    "        out = out.add(x_input)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5af15365",
   "metadata": {},
   "outputs": [],
   "source": [
    "class identity_block(nn.Module):\n",
    "    def __init__(self,t_size,n_step):\n",
    "        super(identity_block,self).__init__()\n",
    "        self.cov1d_1 = nn.Conv2d(t_size,t_size,3,stride=1,padding=1,padding_mode = 'zeros')\n",
    "        self.norm_1 = nn.LayerNorm(n_step)\n",
    "        #self.drop = nn.Dropout(p=0.2)\n",
    "        self.relu = nn.ReLU()\n",
    "#        self.relu = nn.Tanh()\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x_input = x\n",
    "        out = self.cov1d_1(x)\n",
    "        out = self.norm_1(out)\n",
    "        out = self.relu(out)\n",
    "        #output = self.drop(x)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d17dc00",
   "metadata": {},
   "source": [
    "# Rules to use the model architecture \n",
    "### Mask should be included since we use affine transformation function, so the \"num_mask\" should never be \"None\"\n",
    "### The format of mask should be tensor list of the mask, so the fixed_mask should be the list of tenosr "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "00ba2873",
   "metadata": {},
   "outputs": [],
   "source": [
    "# narrow the range of the adjust parameter for the mask region, since it is not the noise free dataset,\n",
    "# this will increase the background noise's influence to the MSE loss\n",
    "# \n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self,original_step_size,pool_list,embedding_size,conv_size,device):\n",
    "        super(Encoder,self).__init__()\n",
    "        \n",
    "        self.device = device\n",
    "        blocks = []\n",
    "        self.input_size_0 = original_step_size[0]\n",
    "        self.input_size_1 = original_step_size[1]\n",
    "        number_of_blocks = len(pool_list)\n",
    "        blocks.append(conv_block(t_size=conv_size, n_step=original_step_size))\n",
    "        blocks.append(identity_block(t_size=conv_size, n_step=original_step_size))\n",
    "        blocks.append(nn.MaxPool2d(pool_list[0], stride=pool_list[0]))\n",
    "        for i in range(1,number_of_blocks):\n",
    "            original_step_size = [original_step_size[0]//pool_list[i-1],original_step_size[1]//pool_list[i-1]]\n",
    "            blocks.append(conv_block(t_size=conv_size, n_step=original_step_size))\n",
    "            blocks.append(identity_block(t_size=conv_size, n_step=original_step_size))\n",
    "            blocks.append(nn.MaxPool2d(pool_list[i], stride=pool_list[i])) \n",
    "            \n",
    "        self.block_layer = nn.ModuleList(blocks)\n",
    "        self.layers=len(blocks)\n",
    "        original_step_size = [original_step_size[0]//pool_list[-1],original_step_size[1]//pool_list[-1]]\n",
    "        \n",
    "        input_size = original_step_size[0]*original_step_size[1]\n",
    "        self.cov2d = nn.Conv2d(1,conv_size,3,stride=1,padding=1,padding_mode = 'zeros')\n",
    "        self.cov2d_1 = nn.Conv2d(conv_size,1,3,stride=1,padding=1,padding_mode = 'zeros')\n",
    "        self.relu_1 = nn.ReLU()\n",
    "        self.relu_2 = nn.ReLU()\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.before = nn.Linear(input_size,20)\n",
    "        self.embedding_size = embedding_size\n",
    "    \n",
    "        self.dense = nn.Linear(20,self.embedding_size)\n",
    "    \n",
    "    \n",
    "    def forward(self,x,rotate_value = None):\n",
    "\n",
    "        out = x.view(-1,1,self.input_size_0,self.input_size_1)\n",
    "        out = self.cov2d(out)\n",
    "        for i in range(self.layers):\n",
    "            out = self.block_layer[i](out)\n",
    "        out = self.cov2d_1(out)\n",
    "        out = torch.flatten(out,start_dim=1)\n",
    "        out = self.before(out) \n",
    "        out = self.dense(out)\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9c731c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "678f105c",
   "metadata": {},
   "outputs": [],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd5a758",
   "metadata": {},
   "source": [
    "# Setting Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ac75025",
   "metadata": {},
   "source": [
    "## Parameters shared with both model architecture "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f99094fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_original_step_size=[200,200]\n",
    "pool_list=[5,4,2]\n",
    "de_original_step_size = [5,5]\n",
    "up_list = [2,4,5]\n",
    "embedding_size=4\n",
    "conv_size =128"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea11fd6c",
   "metadata": {},
   "source": [
    "## Parameters for first Training Process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd2d0f1c",
   "metadata": {},
   "source": [
    "# Set Model Structure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cbe3f57",
   "metadata": {},
   "source": [
    "# Use the generated rotation and scale shear to check on the base position and create new mask region"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f2cc300",
   "metadata": {},
   "source": [
    "## Model for second process training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c88289a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model_2(device,\n",
    "               en_original_step_size = en_original_step_size, \n",
    "               pool_list = pool_list, \n",
    "               de_original_step_size = de_original_step_size,\n",
    "               up_list = up_list,\n",
    "               embedding_size = embedding_size,\n",
    "               conv_size = conv_size,\n",
    "               learning_rate = 3e-5\n",
    "               ):\n",
    "    \n",
    "    encoder = Encoder(original_step_size=en_original_step_size,\n",
    "                      pool_list=pool_list,\n",
    "                      embedding_size=embedding_size,\n",
    "                      conv_size=conv_size,\n",
    "                      device = device).to(device)\n",
    "    \n",
    "    optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "    \n",
    "    return encoder, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ab6fd43b",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder, optimizer = make_model_2(torch.device('cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd5f167d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(p.numel() for p in encoder.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d39907",
   "metadata": {},
   "source": [
    "# Loss Function for Second Process\n",
    "## loss function used for small batch size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90d6cdcd",
   "metadata": {},
   "source": [
    "## loss function used for large batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8a4e5c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function_2nd(join,\n",
    "                      train_iterator,\n",
    "                      optimizer,\n",
    "                      device,\n",
    "                     ):\n",
    "\n",
    "\n",
    "    # set the train mode\n",
    "    join.train()\n",
    "\n",
    "    # loss of the epoch\n",
    "    train_loss = 0\n",
    "    \n",
    "    for x,label in tqdm(train_iterator, leave=True, total=len(train_iterator)):\n",
    "     \n",
    "\n",
    "        x = x.to(device, dtype=torch.float)\n",
    "        label = label.to(device, dtype=torch.float)\n",
    "\n",
    "        # update the gradients to zero\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "\n",
    "        out= join(x)\n",
    "        \n",
    "        \n",
    "        loss  = F.mse_loss(out, label, reduction='mean')\n",
    "\n",
    "\n",
    "        if loss>1.5:\n",
    "\n",
    "            loss = F.l1_loss(out, label, reduction='mean')\n",
    "\n",
    "\n",
    "        if loss>2:\n",
    "            loss=2\n",
    "\n",
    "                \n",
    "        # backward pass\n",
    "        train_loss += loss.item()\n",
    "        loss.backward()\n",
    "        # update the weights\n",
    "        optimizer.step()\n",
    "\n",
    "        return train_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d55ba847",
   "metadata": {},
   "source": [
    "# Second training process\n",
    "### set training parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee81ef44",
   "metadata": {},
   "source": [
    "## RayTune Training Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a23bf882",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = os.path.abspath(\"./polycrystal_output4D.mat\")\n",
    "folder_path = os.path.abspath(\"test_loss_function\")\n",
    "#pretrain_weight = os.path.abspath(\"./04_20_RAYTUNE_lr:0.000065_scale_cof:80.350_shear_cof:16.100_MAE:0.0063_seed:42_epoch:0004_trainloss:0.002384_l1:0.00014_scal:0.00000_shr:0.00000.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7d9f7ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pretrain_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c580e728",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a00f3555",
   "metadata": {},
   "source": [
    "Define the loss landscapes with Hessian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "15ec6a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "from pyhessian import hessian\n",
    "\n",
    "def get_params(model_orig,  model_perb, direction, alpha):\n",
    "    for m_orig, m_perb, d in zip(model_orig.parameters(), model_perb.parameters(), direction):\n",
    "        m_perb.data = m_orig.data + alpha * d\n",
    "    return model_perb\n",
    "\n",
    "def set_params(model_perb, weights_perb):\n",
    "    for m_perb, w_perb in zip(model_perb.parameters(), weights_perb):\n",
    "        m_perb.data = w_perb\n",
    "    return model_perb\n",
    "\n",
    "def get_loss_landscapes(join, train_iterator, optimizer, device, STEPS = 21, DIM = 2, POINTS = 441, START = -2.0, END = 2.0):\n",
    "    \n",
    "    # set the train mode\n",
    "    join.train()\n",
    "\n",
    "    for x,label in tqdm(train_iterator, leave=True, total=len(train_iterator)):\n",
    "     \n",
    "\n",
    "        x = x.to(device, dtype=torch.float)\n",
    "        label = label.to(device, dtype=torch.float)\n",
    "\n",
    "        # update the gradients to zero\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "\n",
    "        model_perb = copy.deepcopy(join)\n",
    "        model_perb.eval()\n",
    "        model_current = copy.deepcopy(join)\n",
    "        model_current.eval()\n",
    "\n",
    "        # make sure the numbers of sampling points is less than the total number of points\n",
    "        FULL_CUBE = False\n",
    "        if POINTS >= STEPS ** DIM:\n",
    "            POINTS = STEPS ** DIM\n",
    "            FULL_CUBE = True\n",
    "            print(\"Full cube calculation. Total points: \", POINTS)\n",
    "        else:\n",
    "            print(\"Random sampling cube calculation. Total points: \", POINTS)\n",
    "        \n",
    "\n",
    "        # Generate the loss values array using BFS\n",
    "        # Create a coordinate array for loss values\n",
    "        loss_coordinates_list = []\n",
    "        pbar = tqdm(total=POINTS, desc=\"Generating 2-D coordinates in the subspace\")\n",
    "        for i in range(STEPS):\n",
    "            for j in range(STEPS):\n",
    "                t = (i,j)\n",
    "                loss_coordinates_list.append(t)\n",
    "                pbar.update(1)\n",
    "        pbar.close()\n",
    "        loss_coordinates = np.array(loss_coordinates_list)\n",
    "\n",
    "        # print(loss_coordinates)\n",
    "        print(loss_coordinates.shape)\n",
    "\n",
    "        # Create a data matrix to store loss values\n",
    "        data_matrix = np.empty([POINTS, 1], dtype=float)\n",
    "        # Fill array with initial value (e.g., -1)\n",
    "        data_matrix.fill(-1)\n",
    "        # print(data_matrix)\n",
    "        print(data_matrix.shape)\n",
    "\n",
    "        criterion = F.mse_loss\n",
    "\n",
    "        # calculate the hessian eigenvalues and eigenvectors\n",
    "        FLAG = True if torch.cuda.is_available() else False\n",
    "        print(\"start creating the hessian computation module\")\n",
    "        hessian_comp = hessian(join, criterion, data=(x, label), cuda=FLAG)\n",
    "        print(\"finish creating the hessian computation module and start calculating the trace\")\n",
    "        trace = hessian_comp.trace()\n",
    "        print(\"The trace of this model is: %.4f\"%(np.mean(trace)))\n",
    "        print(\"finish calculating the hessian trace and start calculating the eigenvecotrs\")\n",
    "        top_eigenvalues, top_eigenvector = hessian_comp.eigenvalues(top_n=DIM)\n",
    "        print(\"finish calculating the eigenvecotrs\")\n",
    "        lams = np.linspace(START, END, STEPS).astype(np.float32)\n",
    "        \n",
    "        # calculate the hessian loss values\n",
    "        for j in tqdm(range(POINTS), desc=\"Calculating sampling loss values in the subspace\"):\n",
    "            # adjust the model and fill with a loss with corresponding model parameters\n",
    "            next_pos = tuple(loss_coordinates[j])\n",
    "            model_current = copy.deepcopy(join)\n",
    "            for i in range(DIM):\n",
    "                model_perb = get_params(model_current, model_perb, top_eigenvector[i], lams[next_pos[i]])\n",
    "                model_current = copy.deepcopy(model_perb)\n",
    "            # calculate the loss value\n",
    "            this_loss = criterion(model_current(x), label, reduction='mean').item()\n",
    "            if this_loss > 1.5:\n",
    "                this_loss = F.l1_loss(model_current(x), label, reduction='mean').item()\n",
    "            if this_loss > 2:\n",
    "                this_loss = 2\n",
    "        \n",
    "            data_matrix[j] = this_loss\n",
    "        \n",
    "        # save the loss values\n",
    "        np.save(folder_path + '/supervised_loss_values.npy', data_matrix)\n",
    "        np.save(folder_path + '/supervised_loss_coordinates.npy', loss_coordinates)\n",
    "        print(\"Loss values saved.\")\n",
    "        # plot the loss values\n",
    "        X, Y = np.meshgrid(np.linspace(START, END, STEPS), np.linspace(START, END, STEPS))\n",
    "        Z = data_matrix.reshape(STEPS, STEPS)\n",
    "        fig, ax = plt.subplots()\n",
    "        ax.contour(X, Y, Z, levels=80)\n",
    "        plt.title(\"Loss landscape for supervised learning of 4DSTEM\")\n",
    "        plt.savefig(folder_path + '/supervised_loss_landscape.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36466c98",
   "metadata": {},
   "source": [
    "# Use Cross Validation and check with training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2d0db6b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(data_, batch_size=64, shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c5302027",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_function( \n",
    "                 dataloader,\n",
    "                 epochs=500,\n",
    "                 learning_rate = 3e-5,\n",
    "                 max_rate = 2e-4,\n",
    "                 up_size = 100,\n",
    "                 epoch_ = None,\n",
    "                 file_path = None,\n",
    "                 folder_path=folder_path,\n",
    "                 best_train_loss= None,\n",
    "                 set_scheduler = False,\n",
    "\n",
    "                ):\n",
    "        \n",
    "            \n",
    "        device = \"cpu\"\n",
    "        if torch.cuda.is_available():\n",
    "            device = \"cuda\"\n",
    "        \n",
    "        \n",
    "        seed = 42\n",
    "        np.random.seed(seed)\n",
    "        torch.manual_seed(seed)\n",
    "        print('.........a step.........')\n",
    "        \n",
    "        patience = 0\n",
    "        \n",
    "        print(\"........successfully load parameters\")\n",
    "        \n",
    "        encoder,optimizer = \\\n",
    "        make_model_2(device,learning_rate = learning_rate)\n",
    "        \n",
    "        radius = 45\n",
    "        \n",
    "            \n",
    "        if set_scheduler:\n",
    "        \n",
    "            lr_scheduler = torch.optim.lr_scheduler.CyclicLR(optimizer, base_lr=learning_rate, max_lr=max_rate,\n",
    "                                                  step_size_up=up_size,cycle_momentum=False)\n",
    "        else: \n",
    "            \n",
    "            lr_scheduler = None\n",
    "        \n",
    "            \n",
    "     \n",
    "        print('..........successfully generate model')\n",
    "        \n",
    "\n",
    "        N_EPOCHS = epochs\n",
    "\n",
    "        if best_train_loss == None:\n",
    "            best_train_loss = float('inf')\n",
    "\n",
    "\n",
    "        if epoch_==None:\n",
    "            start_epoch = 0\n",
    "        else:\n",
    "            start_epoch = epoch_+1\n",
    "        print('...........successfully generate train interator')\n",
    "\n",
    "        for epoch in range(start_epoch,epochs):\n",
    "    \n",
    "            train = loss_function_2nd(encoder,dataloader, optimizer,device)\n",
    "\n",
    "            input_length = len(dataloader)\n",
    "                \n",
    "                \n",
    "            train_loss = train\n",
    "            \n",
    "            train_loss /= input_length\n",
    "\n",
    "    #        VAE_L /= len(train_iterator)\n",
    "            print(f'Epoch {epoch}, Train Loss: {train_loss:.4f}')\n",
    "    #        print(f'......... VAE Loss: {VAE_L:.4f}')\n",
    "            print('.............................')\n",
    "            \n",
    "            # generate the loss landscape for this epoch\n",
    "            get_loss_landscapes(encoder,dataloader, optimizer,device)\n",
    "            \n",
    "            checkpoint = {\n",
    "                \"encoder\":encoder.state_dict(),\n",
    "                'optimizer': optimizer.state_dict(),\n",
    "                \"epoch\": epoch,\n",
    "                'mse_loss': train_loss,\n",
    "            }\n",
    "            if best_train_loss > train_loss:\n",
    "                best_train_loss = train_loss\n",
    "                lr_ = format(optimizer.param_groups[0]['lr'],'.6f')\n",
    "                file_path = folder_path+'/_lr:'+lr_+\\\n",
    "                           f'_epoch:{epoch:04d}_trainloss:{train_loss:.6f}_.pkl'\n",
    "                torch.save(checkpoint, file_path)\n",
    "                \n",
    "\n",
    "            if lr_scheduler!= None:\n",
    "                lr_scheduler.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bfca942",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_function(dataloader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "auto4dstem",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
